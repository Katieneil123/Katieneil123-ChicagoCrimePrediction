{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Crime Prediction Project - 03Feature Engineering & Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: holidays in /opt/anaconda3/lib/python3.12/site-packages (0.67)\n",
      "Requirement already satisfied: python-dateutil in /opt/anaconda3/lib/python3.12/site-packages (from holidays) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil->holidays) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import cdist\n",
    "import holidays\n",
    "import pickle\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Vis Parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data Path\n",
    "DATA_DIR = \"data/\"\n",
    "PROCESSED_DIR = \"data/processed/\"\n",
    "MODELS_DIR = \"models/\"\n",
    "FEATURES_DIR = \"data/features/\"\n",
    "\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_processed_data(filename):\n",
    "    filepath = os.path.join(PROCESSED_DIR, filename)\n",
    "    \n",
    "    if os.path.exists(filepath):\n",
    "        if filename.endswith('.csv'):\n",
    "            return pd.read_csv(filepath, parse_dates=['date'])\n",
    "        elif filename.endswith('.parquet'):\n",
    "            return pd.read_parquet(filepath)\n",
    "        elif filename.endswith('.pkl'):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        else:\n",
    "            print(f\"Unsupported file format: {filename}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "\n",
    "def save_features(data, feature_name, info=None):\n",
    "\n",
    "    filename = f\"{feature_name}.parquet\"\n",
    "    filepath = os.path.join(FEATURES_DIR, filename)\n",
    "    \n",
    "    data.to_parquet(filepath, index=False)\n",
    "    \n",
    "    if info:\n",
    "        info_filepath = filepath.rsplit('.', 1)[0] + '_info.json'\n",
    "        pd.Series(info).to_json(info_filepath)\n",
    "    \n",
    "    print(f\"Features saved to {filepath}\")\n",
    "\n",
    "#------ Temporal Features Development ------#\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    result = df.copy()\n",
    "\n",
    "    if 'date' in result.columns:\n",
    "        result['date'] = pd.to_datetime(result['date'], errors='coerce')\n",
    "        \n",
    "        result['year'] = result['date'].dt.year\n",
    "        result['month'] = result['date'].dt.month\n",
    "        result['day'] = result['date'].dt.day\n",
    "        result['hour'] = result['date'].dt.hour\n",
    "        result['dayofweek'] = result['date'].dt.dayofweek\n",
    "        result['is_weekend'] = result['dayofweek'].isin([5, 6])\n",
    "        \n",
    "        result['season'] = result['month'].map({\n",
    "            12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "            3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "            6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "            9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "        })\n",
    "    \n",
    "    # Add holiday flag\n",
    "    us_holidays = holidays.US()\n",
    "    result['is_holiday'] = result['date'].dt.date.apply(lambda x: x in us_holidays)\n",
    "    \n",
    "    # Add periodic features\n",
    "    if 'hour' in result.columns:\n",
    "        result['hour_sin'] = np.sin(2 * np.pi * result['hour'] / 24)\n",
    "        result['hour_cos'] = np.cos(2 * np.pi * result['hour'] / 24)\n",
    "    \n",
    "    if 'dayofweek' in result.columns:\n",
    "        result['day_of_week_sin'] = np.sin(2 * np.pi * result['dayofweek'] / 7)\n",
    "        result['day_of_week_cos'] = np.cos(2 * np.pi * result['dayofweek'] / 7)\n",
    "    \n",
    "    if 'month' in result.columns:\n",
    "        result['month_sin'] = np.sin(2 * np.pi * result['month'] / 12)\n",
    "        result['month_cos'] = np.cos(2 * np.pi * result['month'] / 12)\n",
    "    \n",
    "    # Add features related to special events\n",
    "    special_events = {\n",
    "        # Holidays and parades\n",
    "        \"St Patrick's Day\": [(3, 17)],\n",
    "        \"Independence Day\": [(7, 4)],  \n",
    "        \"Labor Day\": [(9, 1)],          \n",
    "        \"Halloween\": [(10, 31)],       \n",
    "        \"Thanksgiving\": [(11, 25)],     \n",
    "        \"Christmas\": [(12, 25)],        \n",
    "        \"New Year's Eve\": [(12, 31)],   \n",
    "        # Chicago Special Events\n",
    "        \"Taste of Chicago\": [(7, 10)],             \n",
    "        \"Lollapalooza\": [(8, 1), (8, 2), (8, 3)], \n",
    "        \"Chicago Marathon\": [(10, 10)],            \n",
    "        \"Chicago Air & Water Show\": [(8, 15)]      \n",
    "    }\n",
    "    \n",
    "    result['is_special_event'] = False\n",
    "    result['days_to_nearest_event'] = 365  # Initialize with large value\n",
    "    \n",
    "    for event, dates in special_events.items():\n",
    "        for month, day in dates:\n",
    "            for year in range(result['date'].dt.year.min(), result['date'].dt.year.max() + 1):\n",
    "                try:\n",
    "                    event_date = pd.Timestamp(year=year, month=month, day=day)\n",
    "                    \n",
    "                    # Mark special events\n",
    "                    result.loc[result['date'].dt.date == event_date.date(), 'is_special_event'] = True\n",
    "                    \n",
    "                    # Calculate days to event\n",
    "                    days_diff = abs((result['date'] - event_date).dt.days)\n",
    "                    result['days_to_nearest_event'] = np.minimum(result['days_to_nearest_event'], days_diff)\n",
    "                    \n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "    # Historical Measures\n",
    "    if len(result) > 0:\n",
    "        result = result.sort_values('date')\n",
    "        \n",
    "        crime_counts = result.groupby('date').size().reset_index(name='daily_crime_count')\n",
    "        date_range = pd.date_range(start=crime_counts['date'].min(), end=crime_counts['date'].max())\n",
    "        date_df = pd.DataFrame({'date': date_range})\n",
    "        crime_counts = pd.merge(date_df, crime_counts, on='date', how='left').fillna(0)\n",
    "        \n",
    "        # Calculate Window Crime Counts\n",
    "        crime_counts['crimes_last_7days'] = crime_counts['daily_crime_count'].rolling(7).sum().shift(1)\n",
    "        crime_counts['crimes_last_30days'] = crime_counts['daily_crime_count'].rolling(30).sum().shift(1)\n",
    "        crime_counts['crimes_last_90days'] = crime_counts['daily_crime_count'].rolling(90).sum().shift(1)\n",
    "    \n",
    "        crime_counts['crime_rate_7d_vs_30d'] = (crime_counts['crimes_last_7days'] / 7) / (crime_counts['crimes_last_30days'] / 30)\n",
    "        crime_counts['crime_rate_30d_vs_90d'] = (crime_counts['crimes_last_30days'] / 30) / (crime_counts['crimes_last_90days'] / 90)\n",
    "        \n",
    "        # Fill NaN Values\n",
    "        crime_counts = crime_counts.fillna(method='bfill').fillna(method='ffill').fillna(0)\n",
    "        result = pd.merge(result, crime_counts.drop('daily_crime_count', axis=1), on='date', how='left')\n",
    "    \n",
    "    # Time Anomaly Scores\n",
    "    hourly_crime_rate = result.groupby('hour').size() / result.groupby('hour').size().sum()\n",
    "    result['hour_typicality'] = result['hour'].map(hourly_crime_rate)\n",
    "    \n",
    "    # Normal Crime Rates\n",
    "    day_crime_rate = result.groupby('dayofweek').size() / result.groupby('dayofweek').size().sum()\n",
    "    result['day_typicality'] = result['dayofweek'].map(day_crime_rate)\n",
    "    \n",
    "    result['temporal_anomaly_score'] = (\n",
    "        (1 - result['hour_typicality']) + \n",
    "        (1 - result['day_typicality'])\n",
    "    ) / 2\n",
    "    \n",
    "    return result\n",
    "\n",
    "#------ Spatial Features Development ------#\n",
    "\n",
    "def create_spatial_features(df):\n",
    "\n",
    "    result = df.copy()\n",
    "    \n",
    "    if 'latitude' not in result.columns or 'longitude' not in result.columns:\n",
    "        print(\"Error: Latitude and longitude columns required for spatial feature creation\")\n",
    "        return result\n",
    "    \n",
    "    # Key Locations (Example Coordinates)\n",
    "    key_locations = {\n",
    "        'police_stations': [\n",
    "            (41.8781, -87.6298),  \n",
    "            (41.9742, -87.6644),  \n",
    "            (41.7508, -87.6345),  \n",
    "            (41.8757, -87.7543)  \n",
    "        ],\n",
    "        'transit_stations': [\n",
    "            (41.8781, -87.6340),  \n",
    "            (41.8855, -87.6274),  \n",
    "            (41.8675, -87.6214),  \n",
    "            (41.8768, -87.6317)  \n",
    "        ],\n",
    "        'hospitals': [\n",
    "            (41.8902, -87.6262),  \n",
    "            (41.8689, -87.6680),  \n",
    "            (41.7908, -87.6039)  \n",
    "        ],\n",
    "        'schools': [\n",
    "            (41.8709, -87.6774),  \n",
    "            (41.9220, -87.6513),  \n",
    "            (41.7886, -87.5987)  \n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Calculate Minimum Distance\n",
    "    for location_type, coordinates in key_locations.items():\n",
    "        col_name = f'dist_to_nearest_{location_type}'\n",
    "\n",
    "        result[col_name] = np.inf\n",
    "    \n",
    "        for lat, lon in coordinates:\n",
    "            distances = np.sqrt((result['latitude'] - lat)**2 + (result['longitude'] - lon)**2)\n",
    "            result[col_name] = np.minimum(result[col_name], distances)\n",
    "        \n",
    "        # Replace Infinite Values with Maximum Finite Value\n",
    "        result[col_name] = result[col_name].replace(np.inf, result[col_name][result[col_name] < np.inf].max())\n",
    "    \n",
    "    # Identify Crime Hotspots\n",
    "    coords = result[['latitude', 'longitude']].values\n",
    "    \n",
    "    # Standardize Coordinates\n",
    "    coords_scaled = StandardScaler().fit_transform(coords)\n",
    "    \n",
    "    # DBSCAN Clustering\n",
    "    db = DBSCAN(eps=0.05, min_samples=100).fit(coords_scaled)\n",
    "    result['hotspot_cluster'] = db.labels_\n",
    "    \n",
    "    result['hotspot_cluster'] = result['hotspot_cluster'].replace(-1, -999)\n",
    "    \n",
    "    cluster_counts = result[result['hotspot_cluster'] >= 0]['hotspot_cluster'].value_counts()\n",
    "    result['hotspot_density'] = result['hotspot_cluster'].map(\n",
    "        lambda x: cluster_counts.get(x, 0) if x >= 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Calculate Distance to Nearest Hotspot Center\n",
    "    if len(cluster_counts) > 0:\n",
    "        cluster_centers = {}\n",
    "        for cluster_id in cluster_counts.index:\n",
    "            mask = result['hotspot_cluster'] == cluster_id\n",
    "            if mask.any():\n",
    "                lat_center = result.loc[mask, 'latitude'].mean()\n",
    "                lon_center = result.loc[mask, 'longitude'].mean()\n",
    "                cluster_centers[cluster_id] = (lat_center, lon_center)\n",
    "        \n",
    "        result['dist_to_nearest_hotspot'] = np.inf\n",
    "        for cluster_id, (lat, lon) in cluster_centers.items():\n",
    "            distances = np.sqrt((result['latitude'] - lat)**2 + (result['longitude'] - lon)**2)\n",
    "            result['dist_to_nearest_hotspot'] = np.minimum(result['dist_to_nearest_hotspot'], distances)\n",
    "        \n",
    "        max_dist = result['dist_to_nearest_hotspot'][result['dist_to_nearest_hotspot'] < np.inf].max()\n",
    "        result['dist_to_nearest_hotspot'] = result['dist_to_nearest_hotspot'].replace(np.inf, max_dist)\n",
    "    else:\n",
    "        result['dist_to_nearest_hotspot'] = 0\n",
    "    \n",
    "    # Create Grid ID\n",
    "    lat_bins = pd.cut(result['latitude'], bins=20, labels=False)\n",
    "    lon_bins = pd.cut(result['longitude'], bins=20, labels=False)\n",
    "    result['grid_id'] = lat_bins * 20 + lon_bins\n",
    "    \n",
    "    # Calculate Crime Density\n",
    "    grid_counts = result['grid_id'].value_counts()\n",
    "    result['grid_crime_density'] = result['grid_id'].map(grid_counts)\n",
    "    \n",
    "    grid_density_map = dict(zip(grid_counts.index, grid_counts))\n",
    "    \n",
    "    result['adjacent_grids_density'] = 0\n",
    "    \n",
    "    for grid_id in tqdm(grid_counts.index, desc=\"Computing adjacent grid densities\"):\n",
    "        row = grid_id // 20\n",
    "        col = grid_id % 20\n",
    "        \n",
    "\n",
    "        adjacent_grids = []\n",
    "        for r, c in [(row-1, col), (row+1, col), (row, col-1), (row, col+1)]:\n",
    "            if 0 <= r < 20 and 0 <= c < 20: \n",
    "                adjacent_grid_id = r * 20 + c\n",
    "                adjacent_grids.append(adjacent_grid_id)\n",
    "        \n",
    "        if adjacent_grids:\n",
    "            adjacent_density = sum(grid_density_map.get(g, 0) for g in adjacent_grids) / len(adjacent_grids)\n",
    "            result.loc[result['grid_id'] == grid_id, 'adjacent_grids_density'] = adjacent_density\n",
    "            result.loc[result['grid_id'] == grid_id, 'adjacent_grids_density'] = adjacent_density\n",
    "    \n",
    "    # Calculate Moran's I (Spatial Autocorrelation Index)\n",
    "    k = 5 \n",
    "    result['local_morans_i'] = 0.0\n",
    "    \n",
    "    if len(result) > 10000:\n",
    "        sample_indices = np.random.choice(len(result), 10000, replace=False)\n",
    "        sample = result.iloc[sample_indices]\n",
    "    else:\n",
    "        sample = result\n",
    "    \n",
    "    for idx, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Computing Moran's I\"):\n",
    "        # Calculate Distance to All Other Points\n",
    "        distances = np.sqrt((result['latitude'] - row['latitude'])**2 + \n",
    "                           (result['longitude'] - row['longitude'])**2)\n",
    "        \n",
    "        nearest_indices = np.argsort(distances)[1:k+1]\n",
    "        \n",
    "        neighbor_densities = result.iloc[nearest_indices]['grid_crime_density'].values\n",
    "        \n",
    "        # Calculate Local Moran's I\n",
    "        x_i = row['grid_crime_density']\n",
    "        x_mean = result['grid_crime_density'].mean()\n",
    "        \n",
    "        local_morans_i = (x_i - x_mean) * sum((neighbor_densities - x_mean)) / k\n",
    "        \n",
    "        result.loc[idx, 'local_morans_i'] = local_morans_i\n",
    "    \n",
    "    # For Uncalculated Points, Use the Value of the Nearest Calculated Point\n",
    "    if len(result) > len(sample):\n",
    "    \n",
    "        coords_with_morans = sample[['latitude', 'longitude', 'local_morans_i']].values\n",
    "        \n",
    "        uncomputed_mask = ~result.index.isin(sample.index)\n",
    "        uncomputed_coords = result.loc[uncomputed_mask, ['latitude', 'longitude']].values\n",
    "\n",
    "        for i, (lat, lon) in enumerate(uncomputed_coords):\n",
    "            distances = np.sqrt((coords_with_morans[:, 0] - lat)**2 + \n",
    "                               (coords_with_morans[:, 1] - lon)**2)\n",
    "            nearest_idx = np.argmin(distances)\n",
    "            nearest_morans = coords_with_morans[nearest_idx, 2]\n",
    "            \n",
    "            uncomputed_idx = result.index[uncomputed_mask].values[i]\n",
    "            \n",
    "            result.loc[uncomputed_idx, 'local_morans_i'] = nearest_morans\n",
    "    \n",
    "    return result\n",
    "\n",
    "#------ Crime-Specific Features Development ------#\n",
    "\n",
    "def create_crime_specific_features(df):\n",
    "\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Crime Severity Mapping\n",
    "    severity_mapping = {\n",
    "        'HOMICIDE': 10,\n",
    "        'VIOLENT CRIME': 9,\n",
    "        'ROBBERY': 8,\n",
    "        'THEFT': 6,\n",
    "        'DRUG RELATED': 5,\n",
    "        'PROPERTY CRIME': 4,\n",
    "        'FRAUD': 3,\n",
    "        'OTHER': 2\n",
    "    }\n",
    "    \n",
    "    if 'crime_category' in result.columns:\n",
    "        result['crime_severity'] = result['crime_category'].map(\n",
    "            lambda x: severity_mapping.get(x, 1)\n",
    "        )\n",
    "    else:\n",
    "        result['crime_severity'] = result['crime_type'].apply(\n",
    "            lambda x: severity_mapping.get(x, 1) if x in severity_mapping else 1\n",
    "        )\n",
    "    \n",
    "    # Calculate Threat Level (Based on Severity Score)\n",
    "    result['threat_level'] = pd.cut(\n",
    "        result['crime_severity'], \n",
    "        bins=[0, 3, 6, 10], \n",
    "        labels=['Low', 'Medium', 'High'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    \n",
    "    # Case Attribute Features\n",
    "    if 'is_arrest' not in result.columns and 'arrest' in result.columns:\n",
    "        result['is_arrest'] = result['arrest'].astype(bool)\n",
    "    \n",
    "    if 'is_domestic' not in result.columns and 'domestic' in result.columns:\n",
    "        result['is_domestic'] = result['domestic'].astype(bool)\n",
    "    \n",
    "    if 'police_district' in result.columns and 'is_arrest' in result.columns:\n",
    "        arrest_rates = result.groupby(['police_district', 'crime_category'])['is_arrest'].mean().reset_index()\n",
    "        arrest_rates.columns = ['police_district', 'crime_category', 'historical_arrest_rate']\n",
    "        \n",
    "        result = pd.merge(\n",
    "            result, \n",
    "            arrest_rates, \n",
    "            on=['police_district', 'crime_category'], \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill Missing Values\n",
    "        result['historical_arrest_rate'] = result['historical_arrest_rate'].fillna(\n",
    "            result['is_arrest'].mean()  \n",
    "        )\n",
    "    \n",
    "    # Categorical Variable Conversion\n",
    "    if 'crime_category' in result.columns:\n",
    "        crime_dummies = pd.get_dummies(result['crime_category'], prefix='crime')\n",
    "        crime_dummies = pd.get_dummies(result['crime_category'], prefix='crime')\n",
    "        result = pd.concat([result, crime_dummies], axis=1)\n",
    "    \n",
    "    if 'location_category' in result.columns:\n",
    "        location_dummies = pd.get_dummies(result['location_category'], prefix='loc')\n",
    "        result = pd.concat([result, location_dummies], axis=1)\n",
    "    \n",
    "    if 'threat_level' in result.columns:\n",
    "        threat_dummies = pd.get_dummies(result['threat_level'], prefix='threat')\n",
    "        result = pd.concat([result, threat_dummies], axis=1)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data: 1456714 rows and 26 columns\n"
     ]
    }
   ],
   "source": [
    "df = load_processed_data('chicago_crimes_cleaned.parquet')\n",
    "\n",
    "print(f\"Successfully loaded data: {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Subset or Full Data (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100000 records for feature engineering.\n"
     ]
    }
   ],
   "source": [
    "sample_size = min(100000, len(df))\n",
    "use_full_data = input(f\"Use full dataset ({len(df)} records) for feature engineering? (y/n, default: n): \")\n",
    "\n",
    "if use_full_data.lower() == 'y':\n",
    "    df_sample = df\n",
    "    print(f\"Using full dataset with {len(df)} records.\")\n",
    "else:\n",
    "    df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Using {sample_size} records for feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Features Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 23 new temporal features\n",
      "Saving temporal features...\n",
      "Features saved to data/features/temporal_features.parquet\n"
     ]
    }
   ],
   "source": [
    "df_temp = create_temporal_features(df_sample)\n",
    "print(f\"Added {len(df_temp.columns) - len(df_sample.columns)} new temporal features\")\n",
    "\n",
    "# Save Temporal Features\n",
    "print(\"Saving temporal features...\")\n",
    "save_features(df_temp, 'temporal_features', {\n",
    "    'date_processed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'feature_type': 'Temporal Features',\n",
    "    'record_count': len(df_temp)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Features Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing adjacent grid densities: 100%|██████████| 225/225 [00:00<00:00, 1902.95it/s]\n",
      "Computing Moran's I: 100%|██████████| 10000/10000 [01:51<00:00, 89.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 11 new spatial features\n",
      "Saving spatial features...\n",
      "Features saved to data/features/spatial_features.parquet\n"
     ]
    }
   ],
   "source": [
    "df_spatial = create_spatial_features(df_temp)\n",
    "print(f\"Added {len(df_spatial.columns) - len(df_temp.columns)} new spatial features\")\n",
    "\n",
    "print(\"Saving spatial features...\")\n",
    "save_features(df_spatial, 'spatial_features', {\n",
    "    'date_processed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'feature_type': 'Spatial Features',\n",
    "    'record_count': len(df_spatial)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime-Specific Features Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 19 new crime-specific features\n",
      "Features saved to data/features/all_features.parquet\n"
     ]
    }
   ],
   "source": [
    "df_crime = create_crime_specific_features(df_spatial)\n",
    "print(f\"Added {len(df_crime.columns) - len(df_spatial.columns)} new crime-specific features\")\n",
    "\n",
    "save_features(df_crime, 'all_features', {\n",
    "    'date_processed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'feature_type': 'All Features',\n",
    "    'record_count': len(df_crime),\n",
    "    'total_features': len(df_crime.columns)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying feature engineering to full dataset ---\n",
      "Applying temporal features...\n",
      "Applying spatial features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing adjacent grid densities: 100%|██████████| 229/229 [00:00<00:00, 267.62it/s]\n",
      "Computing Moran's I: 100%|██████████| 10000/10000 [36:13<00:00,  4.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying crime-specific features...\n",
      "Full dataset with features: 1456714 rows and 79 columns\n",
      "Saving full feature dataset...\n",
      "Features saved to data/features/full_dataset_features.parquet\n"
     ]
    }
   ],
   "source": [
    "if len(df_sample) < len(df):\n",
    "    print(\"\\n--- Applying feature engineering to full dataset ---\")\n",
    "    \n",
    "    print(\"Applying temporal features...\")\n",
    "    df_full_temp = create_temporal_features(df)\n",
    "    \n",
    "    print(\"Applying spatial features...\")\n",
    "    df_full_spatial = create_spatial_features(df_full_temp)\n",
    "    \n",
    "    print(\"Applying crime-specific features...\")\n",
    "    df_full = create_crime_specific_features(df_full_spatial)\n",
    "    \n",
    "    print(f\"Full dataset with features: {df_full.shape[0]} rows and {df_full.shape[1]} columns\")\n",
    "    \n",
    "    print(\"Saving full feature dataset...\")\n",
    "    save_features(df_full, 'full_dataset_features', {\n",
    "        'date_processed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'feature_type': 'Complete Feature Set',\n",
    "        'record_count': len(df_full),\n",
    "        'total_features': len(df_full.columns)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
